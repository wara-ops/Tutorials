{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c90a404",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Assignment\n",
    "\n",
    "In this exercise you will work with log data from the Xerces cloud service that is running in the Ericsson Research Data Center (ER DC) in Lund. The goal is to introduce you to the WARA-Ops portal and learn how to work with large data sets from a real-world cloud service.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Running and operating large-scale systems is a highly complex task. Data centers and cloud systems consist of a large number of moving parts. They are designed to be fault-tolerant and survive that components, such as disks, servers, and switch, occasionally break. This is typically handled through redundancy, i.e., using multiple identical subsystems.\n",
    "\n",
    "Data-driven operations are a set of ideas and tools that combine concepts from several fields with the purpose of creating systems that self-manage. The area is becoming essential as modern computing and communication systems rapidly grow in size and complexity; managing these systems by hand is simply no longer possible.\n",
    "\n",
    "However, the systems still break and require manual intervention. Typically this involves scanning through vast amounts of log-files to find out what is causing the outage and identify the root-cause. In the WARA-Ops project we aim at developing technology to do this automatically using for example machine learning.\n",
    "\n",
    "The logs you will be looking were collected over approximately two weeks in the summer of 2023. Something broke around the first of June. We still don't what went wrong. If you find out, please let us known. It's very high volumes of data. The system emits 1200 log items per second.\n",
    "\n",
    "In this assignment we want you to learn how work with real-world data sets, which are typically large, noisy, and undocumented. You should learn how to explore the data and find correlations, anomalies, etc., by for example visualising it or reducing dimensions.\n",
    "\n",
    "### ERDC\n",
    "\n",
    "ER DC and Xerces is operated by a team of IT professionals and adheres to strict Ericsson security and safety policies. This is important to protect potentially sensitive user data. We have been up and running since 2014 and serve many hundreds of researchers.\n",
    "\n",
    "The entire data hall was designed for 2MW, but present, only 300-400kW is consumed (imagine the electricty bill!). The hardware setup on Xerces is approximetly\n",
    "\n",
    "- 300 servers (20+cores, 256+GB RAM)\n",
    "- 16 Nvidia V100 GPUs\n",
    "- 48 Nvidia A100 GPUs\n",
    "- 36 Nvidia GTX 1080 Ti GPUs\n",
    "- 2 PB storage (spinning disc)\n",
    "- 50 TB NVMe storage\n",
    "- Dell 100 Gbps network fabric\n",
    "- External Internet 2x10 Gbps\n",
    "\n",
    "#### OpenStack\n",
    "\n",
    "OpenStack is an open-source cloud computing platform that enables businesses and organizations to deploy and manage their own clouds. It is freely available and can be customized to fit the specific needs of an organization. OpenStack is designed to be scalable and flexible, allowing users to control large pools of compute, storage, and networking resources throughout a datacenter, all managed through a dashboard that gives administrators control while empowering users to provision resources through a web interface.\n",
    "\n",
    "OpenStack consists of a set of independent subsystems or core components. The logs item that you will explore are emitted from theses subsystem. Sometimes you might want to look at the system as a whole and sometimes you want to zoom in on a particular component. Below is an incomplete list of the modules.\n",
    "\n",
    "1. **Nova (Compute)**: Nova is the part of OpenStack that's responsible for managing and automating the pool of compute resources. These resources can be virtual machines or container instances. Nova interacts with the underlying virtualization technologies to manage the lifecycle of compute instances within OpenStack.\n",
    "\n",
    "2. **Swift (Object Storage)**: Swift provides a scalable object storage system that can handle large amounts of unstructured data at a high level of availability. It's designed to store and retrieve any amount of data from anywhere – websites, mobile applications, IoT sensors, etc. It's highly durable and available, ensuring data is always accessible when needed.\n",
    "\n",
    "3. **Cinder (Block Storage)**: Cinder offers persistent block storage to running instances. Its role is akin to that of a traditional hard drive, providing storage to virtual machines but with the flexibility and convenience that comes with virtualized environments. You can think of Cinder as providing the \"hard drives\" for your virtual machines.\n",
    "\n",
    "4. **Neutron (Networking)**: Neutron delivers \"networking-as-a-service\" capabilities in OpenStack. It allows users to create and attach interfaces to networks and provides the API for users to define network connectivity and addressing in the cloud.\n",
    "\n",
    "5. **Horizon (Dashboard)**: Horizon is the graphical interface for OpenStack. It's a web-based dashboard that allows both administrators and users to manage and provision resources within OpenStack. Through Horizon, users can interact with OpenStack's various services without needing to use the command line.\n",
    "\n",
    "6. **Keystone (Identity Service)**: Keystone provides identity services for OpenStack. It's responsible for user management, authentication, and authorization. Essentially, Keystone allows you to control who can do what in OpenStack.\n",
    "\n",
    "7. **Glance (Image Service)**: Glance allows users to discover, register, and retrieve virtual machine images. You can think of it as a repository or library of images that can be used to boot your virtual machines or instances.\n",
    "\n",
    "8. **Heat (Orchestration)**: Heat is a service to orchestrate multiple composite cloud applications using templates, through both an OpenStack-native REST API and a CloudFormation-compatible Query API.\n",
    "\n",
    "These are just some of the core components that make up OpenStack, each playing a crucial role in providing a comprehensive cloud computing platform. OpenStack is highly modular, meaning you can often add or replace components to suit your specific needs, making it a highly versatile choice for cloud infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc24e1a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Using WARA-Ops data and resources\n",
    "\n",
    "WARA-Ops is a portal consisting of two parts. The data catalogue holds the data set. In this assignment you should be using the data set \"ERDClogs-parsed\", which consists of log data collected from the Xerces cloud service during the period 2023-05-21 -- 2023-06-03. The second part of the portal is a JupyterHub notebook compute service. You will find a link to the JupyterHub in the WARA-Ops portal. Please remember to generate a token so that you can access the data.\n",
    "The two parts of the portal are connected using the WARA-Ops DataportalClient. The guide below will show you how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f88cb5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Intro to WARA-Ops, ERDC and the computing environment\n",
    "\n",
    "Since you are reading this, the assumption is that you have already logged into the WARA-Ops portal and managed to start Jupyter. If Jupyter is new to you, take a minute to check out the [documentation][1].\n",
    "\n",
    "Much of the heavy lifting will be done by Pandas, and it is recommended that you keep [Pandas' documentation][2] handliy available in a browser window of its own.\n",
    "\n",
    "We'll start this excercise with a warm-up, followed by an assignment that should be solvable given the warm-up examples, time, and grit.\n",
    "\n",
    "[1]: https://docs.jupyter.org/en/latest/\n",
    "[2]: https://pandas.pydata.org/pandas-docs/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0665821e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Warm-up\n",
    "\n",
    "The first thing to do, is to import Pandas and the client used to access the portal data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82480d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from dataportal import DataportalClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d60e3",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Next, lets's create a couple of handy functions that will let us visualize data. For this we will use [matplotlib][1], sometimes with the aid of [seaborn][2].\n",
    "\n",
    "[1]: https://matplotlib.org\n",
    "[2]: https://seaborn.pydata.org/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec01c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def lineplot(list_x, list_y, title='', xlabel='', ylabel=''):\n",
    "    \"\"\" Plot a line y of x\n",
    "\n",
    "    Given a list of x- and y-values, plot a line.\n",
    "    Optionally, provide title, xlabel and ylabel\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(list_x, list_y)\n",
    "    plt.grid()\n",
    "\n",
    "    _annotate_figure(title, xlabel, ylabel)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def heatmap(df, title='', xlabel='', ylabel=''):\n",
    "    \"\"\" Plot a 2-dimensional \"heatmap\" given a suitable Pandas' dataframe.\n",
    "\n",
    "    Given a suitable Pandas' dataframe, plot a 2-dimensional \"heatmap\".\n",
    "    Optionally, provide title, xlabel and ylabel\n",
    "    \"\"\"\n",
    "    w = len(str(max(df.max()))) * len(df.columns.values) * 0.3\n",
    "    h = len(df) * 0.3\n",
    "    plt.figure(figsize=(w, h))\n",
    "\n",
    "    sns.heatmap(df, annot=True, cmap='coolwarm', fmt='g')\n",
    "\n",
    "    _annotate_figure(title, xlabel, ylabel)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _annotate_figure(title, xlabel, ylabel):\n",
    "    plt.xticks(rotation=45, ha='right', va='top')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed784d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's give the helper functions a quick try, first `lineplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222656ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lineplot(range(1,100), range(1,100), title='Test',  xlabel='x-axis', ylabel='y-axis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccc278d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "then create a \"suitable\" dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede91662",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [1, 2, 3],\n",
    "        \"B\": [4, 5, 6],\n",
    "        \"C\": [7, 8, 9]\n",
    "    }\n",
    ")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f95578",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "that is fed to `heatmap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f700cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd6d42",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Take a moment to recognize the features of the dataframe in the heatmap figure. Now would probably be a good time to skim the documentation [matplotlib][1] and [seaborn][2].\n",
    "\n",
    "\n",
    "[1]: https://matplotlib.org\n",
    "[2]: https://seaborn.pydata.org/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001f8a7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Next, we'll interact with the data-portal, using the `DataportalClient` API. To do so you need the personal `token` from the portal.\n",
    "\n",
    "Paste the token into the cell below, it should end up looking something like:\n",
    "\n",
    "```.py\n",
    "    # paste your token from the portal inside the quotes\n",
    "    token = 'iOiJIUzI1NiIsInR5cCI6IkpXVCJ9.i4iLCJjbj13YXJhb3BzLXVzZXIsY249Z3JvdXBzLGNuPWF4iLCJjbj13YXJhb3BzLXVzZXIsY249Z3JvdXBzLGNuPWF'\n",
    "    dataset = 'ERDClogs-parsed'\n",
    "    client = DataportalClient(token)\n",
    "```\n",
    "\n",
    "Use `help(DataportalClient)` in a python cell to see some documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe41b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste your token from the portal inside the quotes\n",
    "token = ''\n",
    "dataset = 'ERDClogs-parsed'\n",
    "client = DataportalClient(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e12192",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "After executing the above cell, it should print `Connection OK`, which means that the client is authenticated with the system.\n",
    "\n",
    "However, before we can do anything we need to associate the client with a particular dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1604bdac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "client.fromDataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d10f058",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Using the instantiated and configured client, we can examine the files in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e36141",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_iterator = client.listFiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524de504",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Despite its name, `listFiles()` won't return a list of files, but an iteratable object so to use it we have to iterate over it (or force it to a list using `file_list = list(file_iterator)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4fcb8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "for file_info in file_iterator:\n",
    "    print(file_info['FileID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984c19e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "We could pick any of the files, but we'll choose the file with FileID 36666:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a30fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.getData(fileID=36666) # load file content into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce2c86",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Using `df.info()` provides some hints to the dataframe contents…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b2780",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e05289d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "… as does `df.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc716fe",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Although `df.info()` lists most columns as being of type \"object\", `df.head()` suggests that those objects are in fact strings (they are). The columns `domain_id`,\t`user_domain`, and `project_domain` are \"de-indentified\", containing just a single dash (-), and are thus of no interest to us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771cabe",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Different OpenStack modules (see OpenStack section above) annotate log messages slightly different, but we can easily convert them to a \"canonical\" format by replacing e.g. `fatal`, `Fatal`, and `FATAL` with `FATAL` and so on for the other types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_change = {\n",
    "    'fat': 'FATAL',\n",
    "    'err': 'ERROR',\n",
    "    'war': 'WARN',\n",
    "    'inf': 'INFO',\n",
    "    'deb': 'DEBUG',\n",
    "    'tra': 'TRACE'\n",
    "}\n",
    "\n",
    "# Ex 1:\n",
    "def fcn(entry):\n",
    "    # If possible, unify log_level. Defaults to no change.\n",
    "    if pd.notna(entry):\n",
    "        return  dict_change.get(entry[:3].lower(), entry)\n",
    "    return entry\n",
    "\n",
    "df['log_level'] = df['log_level'].apply(fcn)\n",
    "\n",
    "# Ex 2 (if you prefer a one-liner):\n",
    "# df['log_level'] = df['log_level'].apply(lambda x: dict_change.get(x[:3].lower(), x) if pd.notna(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9948117f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Notice how the above code avoided iterating over the entries by using `apply` to _apply_ the same function to a whole column. If you find yourself wanting to loop over the elements of a column, you probably shouldn't…\n",
    "\n",
    "We can e.g. use [`crosstab`][1] to count the number of log entries with a certain ID per [OpenStack module][2] (Nova, Neutron, …):\n",
    "\n",
    "[1]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html#pandas-crosstab\n",
    "[2]: https://www.openstack.org/software/project-navigator/openstack-components#openstack-services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ed6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: crosstab is not able to use columns with NaN entries,\n",
    "#       but `Logger` and `id` does not contain NaN.\n",
    "\n",
    "# Count messages for each message id and Logger\n",
    "df_count = pd.crosstab(index=df['id'], columns=df['Logger'])\n",
    "# Display result as a heatmap using the functions we prepared above\n",
    "heatmap(df_count, title='Number of times log with \"id\" is produced by module \"Logger\"', xlabel='Module', ylabel='message id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2b632",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "It becomes clear from the heatmap that the network-as-a-service module Neutron produces most of the log messages, and messages with id 3385, 3083, and 3084 are the most frequent (in fact, they are _only_ produced by Neutron). Let's take a closer look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return rows with id=3385, 3083, or 3084 AND columns log_level, id, and Payload but only unique messages:\n",
    "df.loc[df['id'].isin([3385, 3083, 3084])][['log_level', 'id', 'Payload']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec620fb",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Hmm, looks like 3385 is the same warning repeated 20000+ times (the boy who cried wolf…), let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c5c8b9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df.loc[df['id'] == 3385][['log_level', 'id', 'Payload']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12962109",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Yup.\n",
    "\n",
    "The messages 3083 and 3084 is the equivalent of a town criers' \"It's 12 o'clock and all is well!'\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['id'].isin([3083, 3084])][['log_level', 'id', 'Payload']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ec30f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "----\n",
    "**Assignment 1**:\n",
    "\n",
    "Neither 3085 nor 3083/3084 are particularly helpful (or interesting) so for an exercise, create a heatmap like the one above but _without_ id 3385, 3083, and 3084 included.\n",
    "\n",
    "_Hint: Check out pandas documentation for [isin][1] and the not operator (~)._\n",
    "\n",
    "[1]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isin.html#pandas.DataFrame.isin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29af3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4486fb",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4039819",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "To visualize data over time, we need to make some preparations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to datetime and set datetime as index\n",
    "if df.index.name != '@timestamp':\n",
    "    df.loc[:, ['@timestamp']] = pd.to_datetime(df['@timestamp'])\n",
    "    df.set_index('@timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec6d93",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "By this, data is [resampled][1] (to get _fewer_ points to plot) such that each sample corresponds to 5 seconds of data:\n",
    "\n",
    "[1]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html#pandas.DataFrame.resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d232148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create samples. (This examples uses 5 seconds per sample).\n",
    "samples = df.resample('5s') # use s, min, H, D\n",
    "\n",
    "# Iterating over samples and count messages for each logger\n",
    "# version 1:\n",
    "list_counts = []\n",
    "for start_datetime, group in samples:\n",
    "    counts = group['Logger'].value_counts()\n",
    "    counts.name = start_datetime\n",
    "    list_counts.append(counts)\n",
    "df_counts = pd.DataFrame(list_counts).fillna(0).astype(int)\n",
    "\n",
    "# version 2: (same thing, but a little convoluted…)\n",
    "# list_counts = [group['Logger'].value_counts().rename(start_datetime) for start_datetime, group in samples]\n",
    "# df_counts = pd.DataFrame(list_counts).fillna(0).astype(int)\n",
    "\n",
    "\n",
    "# now we can plot a heatmap\n",
    "heatmap(df_counts, title='Count of log messages per module per 5s interval', xlabel='Module', ylabel='Time interval')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e090750",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "By chopping the data in timed intervals, we can create plots showing the rate (events/s) of logging in the OpenStack system.\n",
    "\n",
    "First, use `groupby` to create a list of dataframes, each corresponding to the requested time interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711bc12",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "dfs = [g for g in df.groupby(pd.Grouper(freq='s'))]\n",
    "len(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6daa75",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "----\n",
    "**Q:** If you google for e.g. \"chunk pandas dataframe by time interval\", you'll frequently find answers like:\n",
    "\n",
    "`dfs = [g for g in df.groupby(df.index.second)]`\n",
    "\n",
    "Why are those answers wrong (i.e. not the operation you expected)?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fba1e42",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "With the data grouped by time interval, we can plot the velocity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d13171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes dfs[0][1].index.name == '@timestamp'\n",
    "time = []\n",
    "velocity = []\n",
    "for datetime, subframe in dfs:\n",
    "    dt_start = subframe.index[0]\n",
    "    dt_end = subframe.index[-1]\n",
    "    velocity.append(len(subframe)/(dt_end - dt_start).total_seconds())\n",
    "    time.append(datetime)\n",
    "\n",
    "lineplot(time, velocity, title='Average velocity (events/s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fadacf",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "----\n",
    "**Assignment 2**:\n",
    "\n",
    "Plot the log velocity like the one above but _without_ id 3385, 3083, and 3084 included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6227c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3fd34",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b781ba6",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Handling large datasets\n",
    "\n",
    "As a dataset is often in excess of 1GB, it can become a real problem to handle it in a limited amount of memory (RAM), as VMs are often limited to 8GB or 16GB, since working with a dataset expends it, creates copies of data, etc. In short, no matter how much memory you have access to, sooner or later you'll want to analyze data that is too big to fit in memory.\n",
    "\n",
    "While the problem seems unsolvable, all is not lost. The WARA-ops portal (and client) lets you access data in \"chunks\". So far, you have only accessed the first 100 000 entries in the file, but we'll shortly fix that.\n",
    "\n",
    "This is what `help(DataportalClient)` states:\n",
    "\n",
    "    getData(fileID, entries=100000, generator=False)\n",
    "        retrieves data from a file in the selected dataset as a pandas DataFrame\n",
    "\n",
    "By default, a dataframe of 100 000 (default value of `entries`) rows is returned, but if `generator` is set to `True` it will instead return an object that can be iterated over to obtain a sequence of dataframes, each one consisting of `entries` rows. An example illustrates how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = client.getData(fileID=36666, generator=True)\n",
    "for df in dfg:\n",
    "    print(f\"Rows {df.index[0]} to {df.index[-1]}\")\n",
    "    if df.index[-1] > 100000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0ea6a7",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "First of all, the `break` statement is just a way to keep the output from the example short, in general you'd want the complete dataset. Secondly, even though the client and the docs mention _generator_, what is returned from `getData(fileID=fileID, generator=True)` is really an _iterable_ object, a minor quirk that sometimes causes confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e7c78",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "----\n",
    "**Assignment 3**:\n",
    "\n",
    "Now, evaluate a full logfile and produce a heatmap and a velocity plot but exclude all INFO events and messages with id 3385.\n",
    "In other words, your task is to stich together a sequence of dataframes into a single (much larger) dataframe that contains only the columns necessary for plotting, with rows representing INFO events and messages with id 3385 filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b636fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144466d1",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\"",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
