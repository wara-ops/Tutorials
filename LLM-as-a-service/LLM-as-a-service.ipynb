{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d876946c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# LLM-as-a-service\n",
    "\n",
    "This is a quick introduction to using the AI-related services that are part of the WARA-Ops data portal.\n",
    "\n",
    "## What is a Large Language Model?\n",
    "\n",
    "Here's how the LLM-as-a-Service answers the question \"What is an LLM?\":\n",
    "> An LLM (Large Language Model) is a type of artificial neural network designed to process and understand human language. It's essentially a super-smart computer program that can read, write, and converse in multiple languages.\n",
    ">\n",
    "> Think of it like this: Imagine a librarian who has read every book in the library, remembers everything, and can answer any question you ask about the content of those books. That's roughly what an LLM does, but instead of books, it's trained on massive amounts of text data from the internet.\n",
    ">\n",
    "> LLMs are used in applications like language translation, chatbots, and even generating text summaries or entire articles!\n",
    "\n",
    "A little more down-to-earth way to think of it is as a way of predicting the next word in a sentence. Given a starting sentence \"It was a dark\" it might produce \"and\" as the result. Concatenating the result and the initial sentence yields \"It was a dark and\" that can then be fed back to the LLM maybe resulting in \"night\" as the most likely word to follow. That feedbac process can then be repeated until a complete novel is produced.\n",
    "\n",
    "This is of-course an oversimplification, but as a first approximation it is not too bad.\n",
    "\n",
    "Despite what our LLM claims, not only words can be handled by LLMs. Images, sounds, and a lot of other types of data can be handled (FIXME: examples).\n",
    "\n",
    "## What is LLM-as-a-Service?\n",
    "\n",
    "It it a flexible LLM running on powerful GPU-enabled servers that you can access through an API. Most LLM services expose a chat interface that lets you communicate by typing queries (and get anwers) in a web browser. The WARA-ops LLM service is geared towards letting you work with _your_ data the way _you_ want without it ever leaving the portal.\n",
    "\n",
    "## What is it _not_?\n",
    "\n",
    "It is not a general LLM with a friendly GUI.\n",
    "\n",
    "### Supporting services\n",
    "\n",
    "Qdrant vecor database service, see [RAG-tutorial](../RAG-tutorial/intro.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f57ef",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## The basics\n",
    "\n",
    "The LLM is an [Ollama](https://github.com/ollama/ollama) FOSS server that can run can run many pretrained _models_ including DeepSeek that has made quite an impact lately (Jan. 2025).\n",
    "\n",
    "A model in this context is essentially a (large) set of weights obtained by training on vast amounts of data and converted to a standardized format. Take as an example the freely available `llama3.1` model:\n",
    "\n",
    "    Name: llama3.1:latest\n",
    "      Size (MB): 4692.80\n",
    "      Format: gguf\n",
    "      Family: llama\n",
    "      Parameter Size: 8.0B\n",
    "      Quantization Level: Q4_K_M\n",
    "\n",
    "The details are not important at this point, save for the `Parameter Size` that is a rough measure of the models's capability. For more information on the GGUF file format see [this overview](https://huggingface.co/docs/hub/en/gguf)\n",
    "\n",
    "### REST API\n",
    "\n",
    "The service is available (from the portal) at `10.129.20.4:9090` and exposes a REST [API](https://github.com/ollama/ollama/blob/main/docs/api.md)\n",
    "\n",
    "Let's try some basic interaction using the [curl](https://github.com/tldr-pages/tldr/blob/main/pages/common/curl.md) command line tool. By preceeding the command with an exclamation mark (`!`) we can run it from a notebook cell. As a first example we'll just retrieve the server version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f006508",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!curl -s http://10.129.20.4:9090/api/version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf7ba66",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "We can ask the server to list all available models (the `| head -c 500` part truncates the output after 500 characters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26024c77",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!curl -s http://10.129.20.4:9090/api/tags | head -c 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0acffa",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "As you can see, the response is not meant for human consumption and we'll address that shortly, but first we'll show how to save the response to file so that you inspect it Jupyter by clicking the downloaded `response.json` file (in the directory browser to the left):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a95c517",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!curl http://10.129.20.4:9090/api/show -d '{\"model\": \"deepseek-r1:70b\"}' -o response.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3738f1a2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "![The file `response.json` viewed in Jupyter](./img/fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00451d45",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Python client\n",
    "\n",
    "A more convenient way of communicating with the server from a notebook, is by using a [python client](https://github.com/ollama/ollama-python) that wraps the REST API.\n",
    "\n",
    "First the python client must be installed (by running the cell below), and then we can proceed to create an instance and use it to request the list of available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc1455",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!pip -q install ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ee3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "# Create a client for the LLM-as-a-service\n",
    "client = Client(host='10.129.20.4:9090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c538bd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Request the list of models\n",
    "response = client.list()\n",
    "\n",
    "# Format and print the response\n",
    "for model in response.models:\n",
    "  print('Name:', model.model)\n",
    "  print('  Size (MB):', f'{(model.size.real / 1024 / 1024):.2f}')\n",
    "  if model.details:\n",
    "    print('  Format:', model.details.format)\n",
    "    print('  Family:', model.details.family)\n",
    "    print('  Parameter Size:', model.details.parameter_size)\n",
    "    print('  Quantization Level:', model.details.quantization_level)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0a5e6e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## A minimal example\n",
    "\n",
    "Let's request an answer to the question \"Why is the sky blue?\" from the model \"llama3.1:8b\". Note the use of _role_ and _content_ in _messages_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c0f56",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "response = client.chat(model='llama3.1:8b', messages=[\n",
    "    {'role': 'user', 'content': 'Why is the sky blue?'}\n",
    "])\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceefe5a9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Let's see how the much talked about **DeepSeek**-model answers the same question. By picking `deepseek-r1:8b` from the list above, we get a model with _reasoning_ capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a7186",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "response = client.chat(model='deepseek-r1:8b', messages=[\n",
    "    {'role': 'user', 'content': 'Why is the sky blue?'}\n",
    "])\n",
    "# Most of the time, but not always, you get the \"reasoning\" enclosed in <think></think> tags\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a282a5",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Customize\n",
    "\n",
    "We can customize the query by adding a _system_ message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b3fe6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "response = client.chat(model='llama3.1:8b', messages=[\n",
    "    {'role': 'system', 'content': 'You are Mario from Super Mario Bros.'}, \n",
    "    # Bonus credits for changing the above instruction to 'You are Dart Vader.' or Jar-jar Binks if you prefer  \n",
    "    {'role': 'user', 'content': 'Why is the sky blue?'}\n",
    "])\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d84afa",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "Try changing model, question and system instructions to see what happens.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "Try out some examples from the [documentation](https://github.com/ollama/ollama/blob/main/docs/README.md), but be aware that they assume that you are running a local server. With guidance from the above examples you'll be able to figure out any changes required.\n",
    "\n",
    "Remember that the deinitive source of truth regarding parameter etc is the [REST API](https://github.com/ollama/ollama-python/tree/main/examples).\n",
    "\n",
    "### Using portal data\n",
    "\n",
    "An introduction to how to access data from the portal is in the tutorial [PortalAndPandas][1]\n",
    "\n",
    "### Retrieval Augmented Generation (RAG)\n",
    "\n",
    "One way to use a LLM with portal data is through _Retrieval Augmented Generation_ outlined in the [RAG-tutorial][2]\n",
    "\n",
    "### Using your own data\n",
    "\n",
    "Now you should be able to combine the techniques outlined in the tutorials with your own data and your domain knowledge to put the LLM-as-a-service to work for you. Good luck!\n",
    "\n",
    "[1]: ../PortalAndPandas/introduction.ipynb\n",
    "[2]: ../RAG-tutorial/intro.ipynb"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
