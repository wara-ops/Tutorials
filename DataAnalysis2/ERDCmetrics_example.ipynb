{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29a628d",
   "metadata": {},
   "source": [
    "## ERDC metrics data example\n",
    "\n",
    "This notebook provides an example on how to the data in the ERDCmetrics dataset can be imported into a jupyter notebook on the compute portal. Some basic study of the data and clustering are also performed, to give you an idea of how the data could be worked with.\n",
    "\n",
    "Lets start by importing some requried packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf8b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import cluster\n",
    "\n",
    "from dataportal import DataportalClient\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b9511",
   "metadata": {},
   "source": [
    "To import data from the dataset, we must first list the available files. Enter your token and run the segment below to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a7afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ERDCmetrics' # Enter the name of a dataset you have access to\n",
    "token = '' # Enter your token from the dataportal here\n",
    "\n",
    "client = DataportalClient(token)\n",
    "fileList = client.fromDataset(dataset).listFiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e2b8e",
   "metadata": {},
   "source": [
    "The data in the ERDCmetrics dataset is organized into files of events of a specific data type in a given time span. In this example notebook we will only consider the files in a single timespan. Run the segment below to retrieve those files and load them into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b90617",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileids = {\n",
    "    'float': 1,\n",
    "    'str': 112,\n",
    "    'uint': 331\n",
    "}\n",
    "\n",
    "fetched_dataframes = {}\n",
    "for (mtype, id) in fileids.items():\n",
    "    fetched_dataframes[mtype] = client.getData(id, compression='bz2')\n",
    "    print(client.currentLoadedFile())\n",
    "    print(f'From file id {id}')\n",
    "    fetched_dataframes[mtype].info(memory_usage='deep')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd9d0c",
   "metadata": {},
   "source": [
    "The events in the dataframes are organized as rows, where each event contains a timestamp (clock) for a measurement point (name) with the corresponding value (value) for a particular computer (host).\n",
    "\n",
    "Lets inspect the dataframes a bit closer. In particular we are interested in looking at some statistics of the host and name columns, to see how many unique computers and measurement points the dataset contains in the given timespan.\n",
    "\n",
    "Choose a metric types to consider and run the segment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_type = 'float'\n",
    "#metric_type = 'str'\n",
    "#metric_type = 'uint'\n",
    "\n",
    "nbr_of_cols = 10\n",
    "\n",
    "hosts = fetched_dataframes[metric_type].host.value_counts()\n",
    "measurements = fetched_dataframes[metric_type].name.value_counts()\n",
    "\n",
    "print(f'In total there are {len(hosts)} unique hosts in the {metric_type} file.')\n",
    "print(f'Number of entries per host, the {nbr_of_cols} largest:')\n",
    "print(f'{hosts[:nbr_of_cols].to_markdown()}\\n')\n",
    "\n",
    "print(f'In total there are {len(measurements)} unique measurement points in the {metric_type} file.')\n",
    "print(f'Number of entries per measurement point, the {nbr_of_cols} largest:')\n",
    "print(f'{measurements[:nbr_of_cols].to_markdown()}\\n')\n",
    "\n",
    "print(f'The dataframe from the {metric_type} file')\n",
    "display(fetched_dataframes[metric_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d492146f",
   "metadata": {},
   "source": [
    "As the dataframe for the str metric type contains very few entries, and with non-numeric values, it will be disregarded in the remainder of this example. \n",
    "\n",
    "Run the segment below to merge the float and uint dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbfefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.concat([fetched_dataframes['float'], fetched_dataframes['uint']])\n",
    "df_raw.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c3502",
   "metadata": {},
   "source": [
    "Our raw dataframe contains a list of events from different hosts and measurement points, a format not that well suited when performing data analysis with standard python tools. Instead, we will transform it into a more manageable form, with the different measurement points as columns, and each row values for those measurement points in a timespan.\n",
    "\n",
    "This will decrease the granularity of the data, as for some timespans and measurement points there could be multiple events that will have to be aggregated into a single value. Also, for some measurement points there might not be any events in the timespan, which will introduce NaN values that we will have to deal with.\n",
    "\n",
    "For simplicity, we will only consider a single host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6780992",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'eselda06u13'\n",
    "\n",
    "# Extract one host to look at\n",
    "df = df_raw.loc[df_raw['host'] == host, :]\n",
    "\n",
    "# Sort by time\n",
    "df = df.sort_values(by = ['clock', 'ns'], axis=0)\n",
    "\n",
    "# Transform clock from timestamp to datetime, and bin it to minutes\n",
    "df['clock'] = pd.to_datetime(df['clock'], unit='s').dt.floor('min')\n",
    "\n",
    "# Extract the columns corresponding to measurement points, timestamps and values\n",
    "df = df.loc[:,['name', 'clock', 'value']]\n",
    "\n",
    "# Pivot the dataframe to put the measurement points as columns, timestamps as row indices\n",
    "# and the rows the values for each measurement point for a timestamp.\n",
    "df = pd.pivot_table(df, index='clock', columns='name')\n",
    "columns = df.columns\n",
    "second_words = [column[1] for column in columns]\n",
    "df.columns = second_words\n",
    "\n",
    "# Remove columns with many nan values\n",
    "keep = df.isna().sum() < df.shape[0]/2\n",
    "df = df.loc[:, df.columns[keep]]\n",
    "print(f'Remaining NaNs: {df.isna().sum().sum()}')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c7b4a",
   "metadata": {},
   "source": [
    "Lets view some of the columns to get a feel of the data characteristics.\n",
    "\n",
    "Feel free to choose another timespan and measurement points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = 0 # Should be int in [0, 23] \n",
    "t_end = 6 # Should be int in [1, 24]\n",
    "\n",
    "mps = [\n",
    "    'Load average (1m avg)',\n",
    "    'Available memory in %',\n",
    "    'sda: Disk write rate'\n",
    "]\n",
    "\n",
    "t0 = df.index[0] + pd.Timedelta(hours=t_start)\n",
    "tf = df.index[0] + pd.Timedelta(hours=t_end) \n",
    "for mp in mps:\n",
    "    plt.figure()\n",
    "    df[t0:tf][mp].plot()\n",
    "    plt.title(mp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63646fcf",
   "metadata": {},
   "source": [
    "The dataset contains alot of different measurement points for the selected host. Let's perform some dimensionality reduction using [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) to make it more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fbe57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the columns\n",
    "scale = StandardScaler()\n",
    "X = scale.fit_transform(df)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Explained variance')\n",
    "plt.stem(pca.explained_variance_)\n",
    "plt.xlabel('component')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f9b97",
   "metadata": {},
   "source": [
    "The plot above shows how much variance that can be explained in the data by each principal component, in essence giving us an indication on how many columns are actually needed to represent the data. By removing components with low variance we can thus reduce the number of columns in the data to make it more manageable, and still retain a good representation of the original data.\n",
    "\n",
    "We will keep 15 components, feel free to change this.\n",
    "\n",
    "The resulting reduced dataset is still high dimensional. To get a feel of how the data is clustered, we use [t-distribuded Stochastic Neighborhood Embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize it in 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d831f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_to_keep=15\n",
    "\n",
    "pca = PCA(n_components=components_to_keep)\n",
    "X_red = pca.fit_transform(X)\n",
    "\n",
    "# Use TSNE to plot the datapoints and see if there are any obvious clusters\n",
    "tsne = TSNE(init='random', random_state=8)\n",
    "X_tsne = tsne.fit_transform(X_red)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_tsne[:, 0], X_tsne[:, 1], 'C0.')\n",
    "plt.title('TSNE clustering of datapoints in 2D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d510de",
   "metadata": {},
   "source": [
    "As can be seen, there seems to be some distinct clusters available in the data. However, although t-SNE is good for visualizing high dimensional data, it can produce an overly optimistic results on the actual clustering present.\n",
    "\n",
    "The next step is thus to run a clustering algorithm to label the datapoints. We have provided three clustering algorithms via the [scikit-learn clustering](https://scikit-learn.org/stable/modules/clustering.html#clustering) that you could consider. Follow the link to learn more about these, and what more algorithms could be used from the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering = cluster.DBSCAN(eps=5, min_samples=2).fit(X_red)\n",
    "#clustering = cluster.KMeans(n_clusters=8, n_init='auto').fit(X_red)\n",
    "clustering = cluster.AgglomerativeClustering(n_clusters=8).fit(X_red)\n",
    "\n",
    "# Sort labels so that cluster 0 has the most datapoints, cluster 1 second most etc.\n",
    "cluster_labels = np.zeros(len(clustering.labels_), dtype=int)\n",
    "labels, counts = np.unique(clustering.labels_, return_counts=True)\n",
    "U = sorted(tuple(zip(labels, counts)), key=lambda x: x[1], reverse=True)\n",
    "for (new_grp, (old_grp, _)) in enumerate(U):\n",
    "    if old_grp == -1:\n",
    "        # Dont rename the label on datapoints that are not assigned to any cluster\n",
    "        cluster_labels[clustering.labels_ == -1] = -1\n",
    "    else:\n",
    "        cluster_labels[clustering.labels_ == old_grp] = new_grp\n",
    "\n",
    "# Plot the TSNE clustering with the discovered cluster labels\n",
    "clusters_idx = {}\n",
    "for c in set(cluster_labels):\n",
    "    clusters_idx[c] = [i for (i, label) in enumerate(cluster_labels) if label == c]\n",
    "plt.figure()\n",
    "for (i, (c, grp_inds)) in enumerate(clusters_idx.items()):\n",
    "    plt.plot(X_tsne[grp_inds, 0], X_tsne[grp_inds, 1], '.', label=f'cluster {c}')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd0551c",
   "metadata": {},
   "source": [
    "It is of high interest to see how the different discovered clusters differ from each other. One way to do this is to check how the cluster means differ from the dataset mean. In the segment below we have ranked the different measurement points for each cluster based on how much its mean (normalized over the entire dataset) differs from the dataset average (which is 0 for the normalized dataset). Hence, the higher the value, the more they differ. This shows us if there are any obvious groups of measurement points in the cluster that strongly differs from the rest of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = (df-df.mean())/df.std()\n",
    "df_stats = df.describe()\n",
    "\n",
    "clusters = {}\n",
    "for (i, (c, grp_inds)) in enumerate(clusters_idx.items()):\n",
    "    clusters[f'cluster-{c}'] = {\n",
    "        'dataframe': df.iloc[grp_inds],\n",
    "        'stats': df.iloc[grp_inds].describe(),\n",
    "        'diff': pd.DataFrame({\n",
    "            'Norm. Mean': df_norm.iloc[grp_inds].describe().loc['mean'],\n",
    "            'mean_all': df_stats.loc['mean'],\n",
    "            'mean_cluster': abs(df.iloc[grp_inds].describe().loc['mean']),\n",
    "            'CV': df.iloc[grp_inds].describe().loc['std'] / df.iloc[grp_inds].describe().loc['mean']\n",
    "        })\n",
    "    }\n",
    "    clusters[f'cluster-{c}']['diff'].sort_values('Norm. Mean', ascending=False, inplace=True)\n",
    "    clusters[f'cluster-{c}']['diff'].index.rename('Column name', inplace=True)\n",
    "\n",
    "n_imp_cols = 5\n",
    "index = pd.MultiIndex.from_product([clusters.keys(), range(1, 1+n_imp_cols)], names=['Group', 'Rank'])\n",
    "\n",
    "cluster_info = pd.concat([v['diff'].iloc[:n_imp_cols] for v in clusters.values()])\n",
    "cluster_info.reset_index(inplace=True)\n",
    "cluster_info.set_index(index, inplace=True)\n",
    "\n",
    "display(pd.DataFrame({\n",
    "    'datapoints': [v['dataframe'].shape[0] for v in clusters.values()]\n",
    "}, index=clusters.keys()))\n",
    "\n",
    "with pd.option_context('display.max_colwidth', 100):\n",
    "    display(cluster_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ad2b4",
   "metadata": {},
   "source": [
    "Lets plot some timeseries again, but with the cluster labels for the datapoints. Feel free to change the timespan and measurement points to consider. Particularly interesting is to consider the high ranked measurement points for the different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bdbc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = 0 # Should be int in [0, 23]\n",
    "t_end = 24 # Should be int in [1, 24]\n",
    "\n",
    "mps = [\n",
    "    'Load average (1m avg)',\n",
    "    'Available memory in %',\n",
    "    'sda: Disk write rate'\n",
    "]\n",
    "\n",
    "t0 = df.index[0] + pd.Timedelta(hours=t_start)\n",
    "tf = df.index[0] + pd.Timedelta(hours=t_end)\n",
    "for mp in mps:\n",
    "    plt.figure()\n",
    "    plt.title(mp)\n",
    "    plt.plot(df.loc[t0:tf][mp], 'k--')\n",
    "    for (i, (c, c_data)) in enumerate(clusters.items()):\n",
    "        plt.plot(c_data['dataframe'].loc[t0:tf][mp], 'o', label=c)\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
