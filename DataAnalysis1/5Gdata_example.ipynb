{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6487f43",
   "metadata": {},
   "source": [
    "# Load the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ce718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataportal import DataportalClient\n",
    "\n",
    "token = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85640b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(DataportalClient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = DataportalClient(token)\n",
    "files = client.fromDataset('5Gdata').listFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d63048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.getData(list(files)[0]['FileID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda54058",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"CellMeas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c862237",
   "metadata": {},
   "source": [
    "# Let's operate on the dataset here below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install --yes xgboost\n",
    "#!conda install --yes sklearn_evaluation\n",
    "!{sys.executable} -m pip -q install xgboost sklearn_evaluation shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python modules\n",
    "import io\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# external modules\n",
    "#import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn_evaluation import plot\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e086a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # see if filtered file exists:\n",
    "    try:\n",
    "        df = pd.read_csv(\"filtered.csv\", low_memory=False)\n",
    "        return df\n",
    "    except Exception:\n",
    "        print(\"Need to load full file\")\n",
    "\n",
    "    # fix dataset\n",
    "    splits = {}\n",
    "    with open(\"CellMeas.csv\") as f:\n",
    "        splitNo = 0\n",
    "        currentSplit = None\n",
    "        lines = f.readlines()\n",
    "        # print(len(lines))\n",
    "        k = 0\n",
    "        for line in lines:\n",
    "            k += 1\n",
    "            # print(l)\n",
    "            if line.find(\"name\") >= 0:\n",
    "                # print(\"new file\")\n",
    "                splitNo += 1\n",
    "                name = f\"split{splitNo}\"\n",
    "\n",
    "                currentSplit = io.StringIO()\n",
    "                splits[name] = currentSplit\n",
    "\n",
    "            if currentSplit is not None:\n",
    "                currentSplit.write(line)\n",
    "\n",
    "    dsplits = {}\n",
    "    for name, f in splits.items():\n",
    "        f.seek(0)\n",
    "        data = pd.read_csv(f)\n",
    "        f.close()\n",
    "        dsplits[name] = data\n",
    "\n",
    "    df = None\n",
    "    for name, d in dsplits.items():\n",
    "        if df is None:\n",
    "            df = d\n",
    "        else:\n",
    "            df = pd.concat([df, d])\n",
    "\n",
    "    try:\n",
    "        df.to_csv(\"filtered.csv\")\n",
    "    except Exception:\n",
    "        print(\"Failed to write filtered dataframe\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356335b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_dataset(data):\n",
    "    data = data.dropna(axis=1)\n",
    "    # Remove static device info\n",
    "    data = data.loc[:,~data.columns.str.startswith('DEVICE_INFO')]\n",
    "\n",
    "    # drop the \"topic\"\n",
    "    # data = data.drop(columns=['topic'])\n",
    "    data = data.drop(columns=['DATA_FORMAT_VERSION'])\n",
    "    #print(data.head())\n",
    "    # df = drop_strange_values(df, [2147483647])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data):\n",
    "    # time in this data is in nanosecond\n",
    "    # timestamp_nanoseconds = 1644412586263150848.00000\n",
    "    # timestamp_seconds = timestamp_nanoseconds / 1000000000.0\n",
    "    # Replace the timestamp_seconds with your Unix timestamp in seconds\n",
    "    #data['time_second'] = (data['time']/1000000000).round()\n",
    "    data['time_second'] = (data['mTimeStamp']/1000000000).round()\n",
    "    data = data.drop(['mTimeStamp', 'time'], axis=1)\n",
    "    data['date'] = pd.to_datetime(data['time_second'], unit='s')\n",
    "    #print(data['date'])\n",
    "    #quit()\n",
    "    #print('data shape:', data.shape)\n",
    "    #data_downsampled = data.drop_duplicates(subset = 'time_second') # keep only one fo the samples for the same time in second\n",
    "    #data_downsampled = data_downsampled.set_index('date')\n",
    "    #data_downsampled = data_downsampled.sort_index()\n",
    "    #print('data downsampled shape:', data_downsampled.shape)\n",
    "    #plt.plot(data_downsampled['mPingLoss'])\n",
    "    #plt.ylim(0, 250)\n",
    "    #plt.xlim(0, 1000)\n",
    "    #plt.savefig(\"wasp_pingloss_ds.pdf\")\n",
    "    #plt.savefig(\"wasp_pingloss_ds.png\")\n",
    "    #plt.show()\n",
    "\n",
    "    data['second'] = data.date.dt.second\n",
    "    data['minute'] = data.date.dt.minute\n",
    "    data['hour'] = data.date.dt.hour\n",
    "    data['month'] = data.date.dt.month\n",
    "    data['day_of_month'] = data.date.dt.day\n",
    "    data['day_of_year'] = data.date.dt.dayofyear\n",
    "    # data['week_of_year'] = data.date.dt.weekofyear\n",
    "    data['day_of_week'] = data.date.dt.dayofweek\n",
    "    # data['year'] = data.date.dt.year\n",
    "    data[\"is_wknd\"] = data.date.dt.weekday // 4\n",
    "    data['is_month_start'] = data.date.dt.is_month_start.astype(int)\n",
    "    data['is_month_end'] = data.date.dt.is_month_end.astype(int)\n",
    "    data = data.drop(['date'], axis=1)\n",
    "\n",
    "    # Add a new column 'ChangeIndicator' with 1 when CellID changes and 0 otherwise\n",
    "    data['ChangeIndicator_CellID'] = (data['mCellID'] != data['mCellID'].shift()).astype(int)\n",
    "    # Add new column 'Loss' with 1 when the Ping loss is  not zero otherwise when the ping loss is zero it mean there is no loss\n",
    "    data['Loss'] = (data['mPingLoss'] != 0).astype(int)\n",
    "    data = data.drop(['mPingLoss'], axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77587473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(data):\n",
    "    df_categorical_features = data.select_dtypes(include='object')\n",
    "    cat_columns = list(df_categorical_features.columns)\n",
    "\n",
    "    # print(\"Categorical data: {}\".format(cat_columns))\n",
    "\n",
    "    # df = df.select_dtypes(exclude='object')\n",
    "    df_categorical_features = data.select_dtypes(include='object')\n",
    "    cat_columns = list(df_categorical_features.columns)\n",
    "    # print(\"Categorical data: {}\".format(cat_columns))\n",
    "\n",
    "    df_encoded = pd.get_dummies(data, columns=cat_columns, dtype=int)\n",
    "\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6698196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_plot(data, feature):\n",
    "    # Calculating the correlation matrix\n",
    "    correlation_matrix = data.corr()\n",
    "\n",
    "    # Finding highly correlated features with target\n",
    "    highly_correlated_features = correlation_matrix[feature][(correlation_matrix[feature] > 0.4) & (correlation_matrix[feature] < 1.0)]\n",
    "    # Adjust the threshold values (0.4 and 1.0) according to your desired level of correlation\n",
    "\n",
    "    print(f'Highly correlated features with \"{feature}\":')\n",
    "    print(highly_correlated_features)\n",
    "\n",
    "    # Filter the highly correlated features\n",
    "    highly_correlated_data = data[highly_correlated_features.index]\n",
    "\n",
    "    # Plotting the correlation matrix of highly correlated features\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(highly_correlated_data.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(f'Correlation Matrix of Highly Correlated Features with \"{feature}\"')\n",
    "\n",
    "    # Save the figure as an image file (e.g., PNG, JPG, PDF, etc.)\n",
    "    plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')  # Change file format as needed\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff12faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_strange_values(data, values):\n",
    "    # bad_cols = df.columns[data.eq(2147483647).any()]\n",
    "    bad_cols = data.columns[data.isin(values).any()]\n",
    "    for bc in bad_cols:\n",
    "        for v in values:\n",
    "            data = data[data[bc] != v]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, feature, analysis=\"shap\"):\n",
    "    \"\"\"\n",
    "    analysis = \"shap\" | \"pca\" | \"regions\"\n",
    "    \"\"\"\n",
    "    y = data[feature]\n",
    "    X = data.drop([feature], axis=1)\n",
    "\n",
    "    print('y shape {}'.format(str(y.shape)))\n",
    "    print('X shape {}'.format(str(X.shape)))\n",
    "\n",
    "    # ...the class column has to start from 0 (as required since version 1.3.2). An easy way to solve that is using LabelEncoder from sklearn.preprocssing library.\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    # Assuming you have features (X) and labels (y)\n",
    "    # Splitting for each target variable individually\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f'Machine learning model on feature \"{feature}\"')\n",
    "    # Parameters:\n",
    "    # n_estimators (Optional[int]) – Number of boosting rounds.\n",
    "    # max_depth (Optional[int]) – Maximum tree depth for base learners.\n",
    "    # max_leaves – Maximum number of leaves; 0 indicates no limit.\n",
    "    # max_bin – If using histogram-based algorithm, maximum number of bins per feature\n",
    "    # grow_policy – Tree growing policy. 0: favor splitting at nodes closest to the node, i.e. grow depth-wise. 1: favor splitting at nodes with highest loss change.\n",
    "    # learning_rate (Optional[float]) – Boosting learning rate (xgb’s “eta”)\n",
    "    # ......\n",
    "    model = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)\n",
    "    model.fit(X_train, y_train)\n",
    "    print('Accuracy of XGB loss classifier on training set: {:.4f}'.format(model.score(X_train,y_train)))\n",
    "    print('Definitions:  Precision: (true positives) / (true positives + false positives)')\n",
    "    print('              Recall   : (true positives) / (true positives + false negatives)')\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    classification = classification_report(y_test, predictions, zero_division=np.nan)\n",
    "    print(classification)\n",
    "\n",
    "\n",
    "    # Plots\n",
    "    figname = \"\"\n",
    "    title = \"\"\n",
    "    if analysis == \"shap\":\n",
    "        # SHapley Additive exPlanations (SHAP). The goal of SHAP is to explain a machine learning model’s prediction by calculating the contribution of each feature to the prediction.\n",
    "\n",
    "        # check most important features and their contribution to the loss model prediction\n",
    "        shap_values = shap.TreeExplainer(model).shap_values(X_train)\n",
    "        shap.summary_plot(shap_values, X_train, show=False)\n",
    "        #shap.dependence_plot(feature, shap_values, X_train, interaction_index=None, xmax=10, show=False, ax=axes[0])\n",
    "\n",
    "        title = f'SHAP analysis on \"{feature}\" model'\n",
    "        figname = f\"wasp_shap_model_{feature}.png\"\n",
    "\n",
    "\n",
    "    if analysis == \"pca\":\n",
    "        # Classic PCA analysis\n",
    "\n",
    "        orig = MinMaxScaler().fit_transform(X_train)\n",
    "        # 99% of the variance of the original features has been retained\n",
    "        # whiten=True transforms the values of each principal component so that they have 0 mean and 1 variance\n",
    "        pca_data = PCA(n_components=0.99, whiten=True)\n",
    "        pca_fitness = pca_data.fit_transform(orig)\n",
    "        #sns.scatterplot(pca_fitness[:,0], pca_fitness[:,1])\n",
    "        print(\"shape of pca_fitness: {}\".format(str(pca_fitness.shape)))\n",
    "        plt.scatter(pca_fitness[:,0], pca_fitness[:,1])\n",
    "\n",
    "        title = f'PCA analysis on \"{feature}\" model'\n",
    "        figname = f\"wasp_pca_model_{feature}.png\"\n",
    "\n",
    "    if analysis == \"regions\":\n",
    "        title = f'Decisions regions on \"{feature}\" model'\n",
    "        figname = f\"wasp_regions_model_{feature}.png\"\n",
    "        plot_decision_regions(X_train, y_train)\n",
    "\n",
    "    if figname != \"\":\n",
    "        plt.tight_layout()\n",
    "        plt.title(title)\n",
    "        plt.savefig(f\"wasp_shap_model_{feature}.png\", bbox_inches='tight')\n",
    "        #plt.show()\n",
    "\n",
    "    y_score = model.predict_proba(X_test)\n",
    "\n",
    "    return y_test, predictions, y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54670eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, test_idx=None, resolution=0.02):\n",
    "    # make reduction of space\n",
    "    orig = MinMaxScaler().fit_transform(X)\n",
    "    # 99% of the variance of the original features has been retained\n",
    "    # whiten=True transforms the values of each principal component so that they have 0 mean and 1 variance\n",
    "    pca_data = PCA(n_components=2, whiten=True)\n",
    "    reducedX = pca_data.fit_transform(orig)\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "\n",
    "    #color = cm.nipy_spectral(float(i) / n_clusters).reshape(1,-1)\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    print(cmap)\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = reducedX[:, 0].min() - 1, reducedX[:, 0].max() + 1\n",
    "    x2_min, x2_max = reducedX[:, 1].min() - 1, reducedX[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "\n",
    "    # make a model with the reduced space\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reducedX, y, test_size=0.3)\n",
    "    xgb_clf = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)\n",
    "    xgb_clf = xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "    Z = xgb_clf.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4)  #, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=reducedX[y == cl, 0], y=reducedX[y == cl, 1],\n",
    "                    alpha=0.8,\n",
    "                    #c=cmap(idx),\n",
    "                    marker=markers[idx],\n",
    "                    label=cl)\n",
    "\n",
    "    # highlight test samples\n",
    "    if test_idx is not None:\n",
    "        # plot all samples\n",
    "\n",
    "        #X_test, y_test = reducedX[test_idx, :], y[test_idx]\n",
    "\n",
    "        plt.scatter(reducedX[:, 0],\n",
    "                    reducedX[:, 1],\n",
    "                    c='',\n",
    "                    alpha=1.0,\n",
    "                    linewidths=1,\n",
    "                    marker='o',\n",
    "                    s=55, label='test set')\n",
    "\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41281f5c",
   "metadata": {},
   "source": [
    "# Prepare the dataset\n",
    "The dataset will be loaded and cleaned according to some heuristics. Some categorical data (mainly static) is removed and columns containing ```NaN``` are removed. The remaining categorical data is one-hot encoded. Features related to time is added as well as making some columns binary (reducing all positive numbers to ```1```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7417eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains several CSV files. For some reason. Clean it first!\n",
    "df = load_dataset()\n",
    "\n",
    "# Let's save some time and memory, just keep parts of it?\n",
    "df = df.head(400000)\n",
    "\n",
    "print(\"Shape of data prior to pruning: {}\".format(str(df.shape)))\n",
    "df = prune_dataset(df)\n",
    "\n",
    "print(\"Shape of data prior to adding features: {}\".format(str(df.shape)))\n",
    "df = add_features(df)\n",
    "\n",
    "print(\"Shape of data prior to one-hot-encoding: {}\".format(str(df.shape)))\n",
    "df = one_hot_encode(df)\n",
    "\n",
    "# print(\"Currently available columns after hot-encoding: {}\".format(list(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321d7bd",
   "metadata": {},
   "source": [
    "# Produce Correlation Plots\n",
    "\n",
    "Compute pairwise correlation of columns in the dataset. Find the features that correlate the most to the selected feature and inspect how the features in this set are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e36cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation\n",
    "# 'Unnamed: 0', 'time', 'DATA_FORMAT_VERSION', 'mAsuLevel', 'mBW', 'mCellID', 'mCellInfoTS', 'mCellPci', 'mCellSig', 'mCellTac', 'mCqi', 'mCsiSinr', 'mEarcfn', 'mLevel', 'mLocAcc', 'mLocAlt', 'mLocLat', 'mLocLon', 'mNumPings', 'mPingAge', 'mPingAvg', 'mPingLoss', 'mPingMax', 'mPingMin', 'mRegOnCell', 'mRsrp', 'mRsrq', 'mRssi', 'mRssnr', 'mSpeed', 'mSsRsrp', 'mSsRsrq', 'mSsSinr', 'mTimeStamp', 'mTimingAdvance', 'name_CellMeas', 'mCellConn_NONE', 'mCellConn_REQUIRES SDK 28', 'mCellType_GSM', 'mCellType_LTE', 'mCellType_NR', 'mCellType_WCDMA', 'mEventType_CellInfo', 'mEventType_SignalStrength'\n",
    "# correlation_plot(df, 'Loss')\n",
    "# correlation_plot(df, 'mPingAge')\n",
    "# correlation_plot(df, 'mPingAvg')\n",
    "correlation_plot(df, 'mCqi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a67b394",
   "metadata": {},
   "source": [
    "# Apply ML models to perform inference on features\n",
    "Here, for example, ```Loss``` refers to the existence of ping loss and ```ChangeIndicator_CellID``` refers to handover between mobile cells.\n",
    "\n",
    "Parameter ```analyis``` can be either ```\"pca\"```, ```\"shap\"``` or ```\"regions\"```, where\n",
    "\n",
    "* Principal Component Analysis (PCA)\n",
    "    * A method that helps to interprete large datasets containing a high number of dimensions/features per observation by reducing the domanisonality while preserving the maximum amount of information.\n",
    "* SHapley Additive exPlanations (SHAP)\n",
    "    * The goal of SHAP is to explain a machine learning model’s prediction by calculating the contribution of each feature to the prediction.\n",
    "* regions\n",
    "    * Plots the deciions regions for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fdb0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, y_pred, y_score = train_model(df, 'Loss', analysis='shap')\n",
    "# y_test, y_pred, y_score = train_model(df, 'ChangeIndicator_CellID', analysis='pca')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b6c80f",
   "metadata": {},
   "source": [
    "# A confusion matrix represents the prediction summary in matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb574dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "forest_cm = plot.ConfusionMatrix.from_raw_data(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e187b",
   "metadata": {},
   "source": [
    "# Plot Precision/Recall\n",
    "\n",
    "## Precision\n",
    "\n",
    "Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances. Written as\n",
    "\n",
    "$$ \\text{Precision} = {\\text{Relevant retrieved instances} \\over \\text{All retrieved instances}} = { \\text{True positive} \\over \\text{True positive + False positive} } $$\n",
    "\n",
    "\n",
    "\n",
    "## Recall\n",
    "Recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. Written as\n",
    "\n",
    "$$ \\text{Recall} = { \\text{Relevant retrieved instances} \\over \\text{All relevant instances} } = { \\text{True positive} \\over \\text{True positive + False negative} } $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Precision/Recall\n",
    "pr = plot.PrecisionRecall.from_raw_data(y_test, y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d18f0c",
   "metadata": {},
   "source": [
    "# Plot Receiver Operating Characteristic (ROC) curve\n",
    "The ROC curve is the plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold setting. Strive for being  on the \"upper left\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the roc curve\n",
    "roc = plot.ROC.from_raw_data(y_test, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c7890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
