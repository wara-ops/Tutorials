{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0412a100",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# A RAG-enhanced chatbot\n",
    "\n",
    "So, in this example we will build a chatbot backend that will be providing tech support on the GNU make utility<sup>1</sup>.\n",
    "\n",
    "---\n",
    "<p><small>1. The reason for choosing the GNU make utility as the subject is two-fold: 1) the normal (and frequent) use of \"make\" is as a verb and has nothing to do with the noun \"make\" which forces the LLM to make(!) a non-trivial distinctions, and 2) the GNU Make Manual is freely available for download in the context of educational purposes like this. Unfortunately, it is also a case where RAG is somewhat unnecessary due to the abundance of make questions on the internet. Feel free to replace it with something of your own choice.</small></p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee760d3e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "The very first step is to make sure all requirements (in terms of python modules) are satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675edce",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!pip -q install sentence-transformers qdrant-client ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3540b3ba",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## A Basic Chatbot\n",
    "\n",
    "First up is a simple chatbot class that keeps a record of the N latest query/answer pairs generated. Take a minute to appreciate the linguistic complexity of the instructions to the LLM in `sysmsg()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f5902",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "class ChatClient:\n",
    "    \"\"\"A basic chat client that keeps a record of the N latest query/answer pairs generated.\"\"\"\n",
    "\n",
    "    N = 3\n",
    "\n",
    "    def __init__(self, ollama_host, ollama_model):\n",
    "        self.client = ollama.Client(host=ollama_host)\n",
    "        self.model = ollama_model\n",
    "        self.msg_hist = []\n",
    "\n",
    "    def _post(self, query_msg):\n",
    "        # drop oldest query+answer pair from history if there are more than N pairs\n",
    "        self.msg_hist = self.msg_hist[-2*self.N:]\n",
    "        # add the current query\n",
    "        self.msg_hist.append(query_msg)\n",
    "        # prepend the generic instructions\n",
    "        msg_list = [self.sysmsg()] + self.msg_hist\n",
    "        # print(msg_list)\n",
    "\n",
    "        response = self.client.chat(\n",
    "            model=self.model,\n",
    "            messages=msg_list,\n",
    "            stream=False,\n",
    "        )\n",
    "        reply = response['message']\n",
    "        self.msg_hist.append(reply)\n",
    "        return reply['content']\n",
    "\n",
    "\n",
    "    def sysmsg(self):\n",
    "        # a template for the instructions to the system\n",
    "        return {\n",
    "            'role': 'system',\n",
    "            'content': '''\n",
    "            You are an AI assistant providing tech support on the GNU Make program.\n",
    "            In this context GNU Make, Make, and 'make' always refer to the GNU Make program,\n",
    "            and so does the noun make.\n",
    "            Provide short, concise answers prefixed by >>> .\n",
    "            If you cannot answer the question, just say so.\n",
    "            '''.strip(),\n",
    "        }\n",
    "\n",
    "    def post(self, query):\n",
    "        # posting a query using a simple template, receiving (and printing) the answer\n",
    "        msg = {\n",
    "            'role': 'user',\n",
    "             'content': f'{query}',\n",
    "        }\n",
    "        answer = self._post(msg)\n",
    "        print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2dd95",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "After instantiating a chatbot ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9e2e6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "OLLAMA_HOST = 'http://10.129.20.4:9090'\n",
    "OLLAMA_MODEL = 'llama3:70b'\n",
    "\n",
    "client = ChatClient(OLLAMA_HOST, OLLAMA_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacefe8f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "... we can pose it a series of questions. Note that there is less and less explicit context in the questions, with the last one only implicitly referring to the core question (compiling latex into pdf). See the screenshot below in case the conversation goes\n",
    "haywire...\n",
    "<img src=\"img/chatbot1.png\" alt=\"Chatbot\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47fce1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "client.post(\"What can I use make for?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa2abee",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "client.post(\"Can you give a simple example of how to compile a single-file C project?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3505482",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "client.post(\"What would a similar example of compiling latex into pdf look like?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d151d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.post(\"but I don't have pdflatex on my system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad8505",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Preparing content for RAG\n",
    "\n",
    "This has been explained in the previous tutorials, and will thus be stated without much commentry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c325ab6d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Chunking\n",
    "\n",
    "Rather than just taking sentences as chunks, we'll bundle bundle a number of sentences into chunks, respecting the (approximate) maximum chunksize (`CHUNKSIZE=1200`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4908ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Split the input data into chunks\n",
    "#\n",
    "import re\n",
    "\n",
    "class Manual:\n",
    "\n",
    "    CHUNKSIZE=1200\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        with open(filepath, 'r') as fd:\n",
    "            text = fd.read()\n",
    "        self.sentences = re.split(r\"(?:(?<=\\.|\\?|!)\\n)|(?:\\n(?=\\s*\\d{1,}(?:\\.\\d)*))|(?:\\n\\n)\", text)\n",
    "\n",
    "    def _chunk_sentences(self, idx):\n",
    "        chunk = \"\"\n",
    "        while len(chunk) < self.CHUNKSIZE and idx < len(self.sentences):\n",
    "            chunk = chunk + \"\\n\" + self.sentences[idx]\n",
    "            idx += 1\n",
    "        return chunk, idx\n",
    "\n",
    "    def chunk_text(self):\n",
    "        metadata = []\n",
    "        chunks = []\n",
    "        for first in range(0, len(self.sentences), 3):\n",
    "            chunk, last = self._chunk_sentences(first)\n",
    "            chunks.append(chunk)\n",
    "            metadata.append({\"first\": first, \"last\": last})\n",
    "\n",
    "        return chunks, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd581e99",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Embedding\n",
    "\n",
    "This is straightforward, except that embeddings are cached to save time on repeated runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5346f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class Embedder:\n",
    "\n",
    "    model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "    def __init__(self):\n",
    "        # Get the model\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "        self._cache = []\n",
    "        self._hash = None\n",
    "\n",
    "    def embed(self, chunks, force_compute = False):\n",
    "        # Vectorize, i.e. create embeddings\n",
    "        # This can take a couple of minutes,\n",
    "        # so use cached embeddings unless something changed\n",
    "        _hash = hash((tuple(chunks), self.model_name))\n",
    "        compute = len(self._cache) == 0 or self._hash != _hash or force_compute\n",
    "        if compute:\n",
    "            self._hash = _hash\n",
    "            self._cache = self.model.encode(chunks, show_progress_bar=True)\n",
    "        return self._cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1a488",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Vector store\n",
    "\n",
    "**Please note that you must define a name for the collection below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa8422",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client import models\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "class VectorStore:\n",
    "\n",
    "    # Provide a name for the _collection_ making up your corner of the database\n",
    "    # use e.g. <signum>_gmm_date\n",
    "    collection_name = \n",
    "\n",
    "    def __init__(self, host, port, embedding_model):\n",
    "        # Create a client connecting to the service\n",
    "        self.db = QdrantClient(host=host, port=port)\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def _clear(self):\n",
    "        # Check if collection (for this toy example) already exist, and remove if so\n",
    "        if self.db.collection_exists(collection_name=self.collection_name):\n",
    "           self.db.delete_collection(collection_name=self.collection_name)\n",
    "\n",
    "        # Create a named collection and set vector dimension and metric (EUCLID => L2)\n",
    "        self.db.create_collection(\n",
    "            collection_name = self.collection_name,\n",
    "            vectors_config = VectorParams(\n",
    "                size=self.embedding_model.get_sentence_embedding_dimension(),\n",
    "                distance=Distance.EUCLID\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Upload our embeddings, one variant of many (destroys old data)\n",
    "    def upload(self, embeddings, metadata = None):\n",
    "        self._clear()\n",
    "        # If ids are not provided, Qdrant Client will generate random UUIDs for each entry\n",
    "        n = len(embeddings)\n",
    "        self.db.upload_collection(\n",
    "            collection_name = self.collection_name,\n",
    "            ids = range(n),\n",
    "            payload = metadata,\n",
    "            vectors = embeddings,\n",
    "        )\n",
    "\n",
    "    def query(self, question, n = 1):\n",
    "        query_embedding = self.embedding_model.encode(question)\n",
    "        # Return the two closest matches\n",
    "        search_results = self.db.search(\n",
    "            collection_name = self.collection_name,\n",
    "            search_params = models.SearchParams(hnsw_ef=10, exact=False),\n",
    "            query_vector = query_embedding,\n",
    "            limit = n,\n",
    "        )\n",
    "\n",
    "        return [(result.id, result.payload, result.score) for result in search_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb93cf84",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Consuming the manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffcdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = Manual('gmm/gnu-make-manual.txt')\n",
    "chunks, _ = gmm.chunk_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a331e5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "embedder = Embedder()\n",
    "embeddings = embedder.embed(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855704fa",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Test caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedder.embed(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b77dad",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Uploading to the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb147df",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORSTORE_HOST = \"10.129.20.4\"\n",
    "VECTORSTORE_PORT = 6333\n",
    "\n",
    "store = VectorStore(VECTORSTORE_HOST, VECTORSTORE_PORT, embedder.model)\n",
    "store.upload(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2695e3f6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Testing the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706a2cb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "res = store.query(\"How can I use make to compile a program?\", 3)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb85990",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "for idx, _, _ in res:\n",
    "    print(f\"{idx}:\\n{chunks[idx]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36babba",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Building a RAG-enabled chat client\n",
    "\n",
    "Finally, we have to subclass the chat client to augment(!) it to accommodate the RAG context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6843cf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class RAGChatClient(ChatClient):\n",
    "    \"\"\"A RAG chat client that keeps a record of the N latest query/answer pairs generated and provides a RAG context.\"\"\"\n",
    "\n",
    "    CTX_SIZE = 3\n",
    "\n",
    "    def __init__(self, ollama_host, ollama_model, vectorstore_host, vectorstore_port, chunks, embedding_model):\n",
    "        super().__init__(ollama_host, ollama_model)\n",
    "        self.store = VectorStore(vectorstore_host, vectorstore_port, embedding_model)\n",
    "        self.chunks = chunks\n",
    "\n",
    "    def sysmsg(self):\n",
    "        # a template for the instructions to the system with a CONTEXT placeholder\n",
    "        return {\n",
    "            'role': 'system',\n",
    "            'content': '''\n",
    "            You are an AI assistant providing tech support on the GNU Make programs.\n",
    "            Here GNU Make, Make, and 'make' always refer to the GNU Make program,\n",
    "            and so does the noun make. Take the provided context into account in your answers.\n",
    "            Provide references to the context where appropriate.\n",
    "\n",
    "            Provide short, concise answers prefixed by >>> .\n",
    "            If you cannot answer the question, just say so.\n",
    "\n",
    "            Context:\n",
    "            {self.context}\n",
    "            '''.strip(),\n",
    "        }\n",
    "\n",
    "    def post(self, query):\n",
    "        # get RAG context\n",
    "        res = store.query(query, self.CTX_SIZE)\n",
    "        ctxs = [f\"{idx}: {self.chunks[idx]}\" for idx, _, _ in res]\n",
    "        self.context = \"\\n\".join(ctxs)\n",
    "        # Reusing `post()` for posting a query using a template, receiving (and printing) the answer\n",
    "        super().post(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb1c33",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## A RAG-enabled chat client\n",
    "\n",
    "With that we can try out the improved chat client. Let's test it with the same sequence of questions as previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f67d50",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "client = RAGChatClient(OLLAMA_HOST, OLLAMA_MODEL, VECTORSTORE_HOST, VECTORSTORE_PORT, chunks, embedder.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0c53f9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "client.post(\"What can I use make for?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2444ac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "client.post(\"Can you give a simple example of how to compile a single-file C project?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8950e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "client.post(\"What would a similar example of compiling latex into pdf look like?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c62866a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "client.post(\"but I don't have pdflatex on my system\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
