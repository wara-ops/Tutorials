{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c056fe",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# RAG with langchain\n",
    "\n",
    "The purpose of this exercise is to show how using a tool like LangChain is different from the bare-bones implementation of the previous tutorial and the potential benefits of such a tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4092db",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "For this notebook we need two more Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0215c6e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!pip -q install langchain langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243de08",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Baseline\n",
    "\n",
    "Before getting into RAG with LangChain, let's again establish a baseline by querying our LLM without using RAG. Note that LangChain is essentially a programming environment that wraps all of our other tools, for good or bad. So, instead of importing the client API using `from ollama import Client` as in the previous part, we get a module from LangChain's community…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "ollama_host = 'http://10.129.20.4:9090'\n",
    "ollama_model = 'llama3:70b'\n",
    "\n",
    "# Simple chain invocation\n",
    "llm = OllamaLLM(model=ollama_model, base_url=ollama_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2df41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an AI assistant. Your task is to understand the user question, and provide an answer.\n",
    "\n",
    "            Your answers are short, to the point, and written by an domain expert.\n",
    "            If you don't know the answer, simply state, \"I don't know\"\n",
    "            \"\"\"\n",
    "        ),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b3859",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Here comes the part from which LangChain takes its name: parts of the application are \"chained\" together using a syntax reminding of UNIX pipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eeafe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "query = \"What is special about HackerNews?\"\n",
    "response = chain.invoke({\"user_input\": query})\n",
    "print(response)\n",
    "\n",
    "# Verify that the answer is \"I don't know\"\n",
    "# query = \"What do you know about mr. Mjptkck?\"\n",
    "# response = chain.invoke({\"user_input\": query})\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c49f343",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Most tutorials add an output parser at the end of the chain, but in this case it is simply a passthrough, adding nothing of value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc70088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output = StrOutputParser() # Basically a NOP is this example\n",
    "\n",
    "chain = prompt | llm | output\n",
    "\n",
    "query = \"What is special about HackerNews?\"\n",
    "response = chain.invoke({\"user_input\": query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b9cf0",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Chunking\n",
    "\n",
    "For the sake of comparison, we will use the plain python code example from the previous part to split the data into senteces (chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffaf7d6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Split the input data into sentence-sized chunks\n",
    "#\n",
    "import re\n",
    "import json\n",
    "\n",
    "chunks = []\n",
    "index = 0\n",
    "\n",
    "filenames = [\"newsfaq.json\", \"newsguidelines.json\", \"security.json\", \"legal.json\"]\n",
    "# Iterate over the entries in data/ and read each JSON file in turn\n",
    "for filename in filenames:\n",
    "    filepath = f\"./data/{filename}\"\n",
    "    with open(filepath) as fd:\n",
    "        data = json.load(fd)\n",
    "\n",
    "    url = data['url']\n",
    "    text = data['content']\n",
    "    # Split the file's text contents into sentences using python regex:\n",
    "    #   A sequence of characters is deemed a sentence if followed by a\n",
    "    #   full stop (.), question mark (?), or an exclamation mark (!)\n",
    "    #   immediately followed by one or more whitespaces.\n",
    "    sentences = re.split(r\"(?<=\\.|\\?|!)\\s+\", text)\n",
    "    # Each sentence make up a chunk, store it with references (url and id)\n",
    "    for sentence in sentences:\n",
    "        chunks.append({'id': index, 'text': sentence, 'url': url})\n",
    "        index += 1\n",
    "\n",
    "# Write the resulting array to file:\n",
    "with open('chunks.json', 'w') as fd:\n",
    "    json.dump(chunks, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5647b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a sanity check, it should be ~570 chunks\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b57022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the sentences from our chunks\n",
    "sentences = [chunk['text'] for chunk in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf2c8c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Embedding and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f645dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a named _collection_ making up our corner of the database (it is a shared resource)\n",
    "collection_name =\n",
    "# collection_name = \"my_hackernews_250101\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cfb9a9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "This is where LangChain IMHO gets a wee bit ugly as we must load yet another\n",
    "module tied to the implementation (_leaky abstraction_) and we must install `langchain-huggingface`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48baeda",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!pip -q install langchain-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75be1ea",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Then we can continue to create the embeddings using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd03758",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "embeddings_model = HuggingFaceEmbeddings(show_progress=False) # Change to True for visual feedback\n",
    "embeddings = embeddings_model.embed_documents(sentences)\n",
    "\n",
    "print(len(embeddings), len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91316f15",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Ignore warnings about `tqdm` etc., nothing to do about it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6257b392",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "The \"unit\" that LangChain is working with is `Document`, so we'll first have to wrap all chunks/sentences in `Document`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355dca6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [Document(page_content=sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fa5c8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Again, we'll have to bite the bullet and install LangcChain modules specific to an external service type (Qdrant database), another example of leaky abstractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992d7da",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!pip -q install langchain-qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf4244",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Then we can continue to create the embeddings using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9f5a34",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "LangcChain will attach a default id to each document as it is uploaded to qdrant, but we'll be providing integer ids (index of the sentence) to prevent that from happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b5bac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# https://python.langchain.com/v0.2/docs/integrations/vectorstores/qdrant/\n",
    "# https://api.python.langchain.com/en/latest/qdrant/langchain_qdrant.qdrant.QdrantVectorStore.html\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    documents,\n",
    "    embeddings_model,\n",
    "    url=\"http://10.129.20.4:6333\",\n",
    "    distance='Euclid',\n",
    "    collection_name=collection_name,\n",
    "    ids=list(range(len(documents))),\n",
    "    force_recreate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43638023",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "There are two things to note here, besides passing the documents (embeddings) and the requested ids, which is:\n",
    "\n",
    "1. Setting `force_recreate=True`, which is simply a convenience in a toy example like this, and\n",
    "2. using the string `'Euclid'` to define the distance metric used,\n",
    "\n",
    "The second point deserves some explanation:\n",
    "Using LangChain's own `EmbeddingDistance.EUCLIDEAN` (`from langchain.evaluation import EmbeddingDistance`) results in an error as it evaluates to the string `'euclidean'`, and the officially recommended solution is to import Qdrant's own definition using `from qdrant_client.models import Distance` and use `Distance.EUCLID` which evalutes to `'Euclid'`. Now, that could be the most blatant example of a _leaky abstraction_ that I've ever seen, and the world is a much better place if that wart is ignored and the literal `'Euclid'` is used instead.\n",
    "\n",
    "<!--\n",
    "# https://api.python.langchain.com/en/latest/qdrant/langchain_qdrant.qdrant.QdrantVectorStore.html#langchain_qdrant.qdrant.QdrantVectorStore\n",
    "# from langchain.evaluation import EmbeddingDistance\n",
    "# from qdrant_client.models import Distance, VectorParams # LEAKY ABSTRACTION (from official docs)\n",
    "\n",
    "# ValidationError: 1 validation error for VectorParams\n",
    "# distance\n",
    "#   Input should be 'Cosine', 'Euclid', 'Dot' or 'Manhattan' [type=enum, input_value=<EmbeddingDistance.EUCLIDEAN: 'euclidean'>, input_type=EmbeddingDistance]\n",
    "#     For further information visit https://errors.pydantic.dev/2.8/v/enum\n",
    "#\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf619f90",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div style=\"background-color:lightblue; padding:5px\">\n",
    "\n",
    "**Sidenote**: Using an existing collection\n",
    "\n",
    "To use an instance of `langchain_qdrant.Qdrant` on an _existing_ collection without loading any new documents or texts, you can use the `Qdrant.from_existing_collection()` method.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcdb44c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Anyhow, now we can retrive chunks from the database using ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e7bf69",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "vector_store.get_by_ids([5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a133d641",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Let's do a quick sanity check like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593c2f1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "query = sentences[5]\n",
    "print(query)\n",
    "embedded_query = embeddings_model.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15cef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search_with_score(\n",
    "    query=query, k=2\n",
    ")\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8aeac9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import math\n",
    "math.sqrt(sum(((x-y)*(x-y) for x,y in zip(embedded_query, embedded_query))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a2db7",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "With some luck, the result of the distance computation should be 0, just as reported by the `similarity_search_with_score` method for the first result. Now, let's do the same for the second result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc593580",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "embedded_id6 = embeddings_model.embed_query(sentence[6])\n",
    "math.sqrt(sum(((x-y)*(x-y) for x,y in zip(embedded_query, embedded_id6))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321f471",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "If your computation agrees with the score returned by _LangChain_ congratulations. As of the time of writing the figure reported back by `similarity_search_with_score` is neither the euclidean distance nor its squared value (which makes sense from a numerical perspective).\n",
    "It is hard to figure out what quantity LangChain returns here, but it is not consistent across backends, see <https://github.com/langchain-ai/langchain/issues/4517>. We can just hope that is consistent with the L2-distance (spoiler: it is not. Pick e.g. the 100th returned result, which has a smaller L2-distance then the 3rd result).\n",
    "\n",
    "Ah, well, let's get on with our lives…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35178be1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "In order to chain together the vector database with the LLM we need to configure a `retriever` object rather than just a database client (see e.g. <https://python.langchain.com/docs/how_to/vectorstore_retriever/>).\n",
    "\n",
    "The search_type is set to `\"similarity\"` (the default) and the number of requested hits are stated in a (kludgy) search kwargs blindly passed down to the underlying database client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6eca8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04a7c8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## LangChain RAG\n",
    "\n",
    "Putting it together in a complete RAG example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61118120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an AI assistant. Your task is to understand the user question, and provide an answer.\n",
    "\n",
    "            Your answers are short, to the point, and written by an domain expert.\n",
    "            If you don't know the answer, simply state, \"I don't know\".\n",
    "\n",
    "            Use the following pieces of retrieved context to answer the question.\n",
    "\n",
    "\n",
    "            {context}\n",
    "            \"\"\"\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706beaae",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we can pose a question to the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a342634e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "query = 'What is special about HackerNews?'\n",
    "result = rag_chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b0d5a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "The result returned has three parts; the query, the retrieved context, and the answer from the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f779c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b95509",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1719ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de7913",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Now, the above example does not live up to the LangChain name, so we could rewrite it in a \"LangChain-y\" style (see <https://python.langchain.com/docs/tutorials/rag/>).\n",
    "NB. Reuse of `retriever`, `prompt`, and `llm` from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb13d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_context(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_context, \"input\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser() # Can be left out in this example\n",
    ")\n",
    "\n",
    "answer = rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d06b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580c1790",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "So, what benefits does LangChain bring to the table? I'm not sure. For an example such as this, I would say that it is of no use. Problem is that it is not difficult to find critical comments on its usefulness in large scale deployments (see e.g. [why we no longer use LangChain for building our AI agents](https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents#:~:text=The%20problem%20with%20LangChain's%20abstractions,understand%20and%20frustrating%20to%20maintain.)). Maybe there is some middle ground where it is useful, I don't know. Probably depends on the use case. YMMV.\n",
    "\n",
    "At least I would not recommend starting out a new project based on LangChain from day one, but rather migrating to it after getting to understand the problem and making a proper cost/benefit analysis."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
