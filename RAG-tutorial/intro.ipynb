{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c900a6c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "# RAG From Scratch â€“ a Three Part Tutorial\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is intended to alleviate some of the most obvious [problems][1] displayed by Large Language Models (LLMs), such as hallucination (where an LLM starts making up \"facts\" and stating them as such) and not being able to reference the source for its claims. RAG consists of three steps; *retrieval* of context-specific information, *augmenting* (i.e. adding context to) the LLM [prompt][2], and letting the LLM generate an answer taking that context into account, providing references to the context.\n",
    "\n",
    "The tutorials build upon the previous ones, so even if you know the subject of tutorial it is recommended that you browse through it rather than skip it.\n",
    "\n",
    "1. [A bare-bones RAG implementation][3] using as little magic as possible to explore the fundamental concepts using an illustrative toy example\n",
    "2. [RAG with LangChain][4] utilizes the popular LangChain environment to implement the same toy example as in the previous tutorial.\n",
    "3. [A RAG-enhanced chatbot][5] builds the core of a fully functional chatbot that get help from RAG to stay on track.\n",
    "\n",
    "[1]: https://youtu.be/T-D1OfcDW1M?si=nKf8KC93tcsbbAlO\n",
    "[2]: https://medium.com/thedeephub/llm-prompt-engineering-for-beginners-what-it-is-and-how-to-get-started-0c1b483d5d4f\n",
    "[3]: hn+scratch.ipynb\n",
    "[4]: hn+langchain.ipynb\n",
    "[5]: chatbot+RAG.ipynb"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
